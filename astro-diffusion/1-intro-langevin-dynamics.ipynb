{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# Introduction to Diffusion Models and their connection to Langevin Dynamics\n",
    "\n",
    "Before we take a look at actual larger scale \"Diffusion Models\" (DM), we study a simple toy model or one dimensional case of a DM. DMs in general are tightly connected to an approach named \"Langevin Dynamics\", which was introduced by Max Welling et al. in 2012 in \"Bayesian Learning via Stochastic Gradient Langevin Dynamics\", as a way to sample from high dimensional distributions in a Bayesian learning setting.\n",
    "\n",
    "Leaving aside details on proof of convergence etc. the overall idea is very simple. Imagine any standard learning problem, that is solved with maximum likelihood and (stochastic) gradient decent (SGD). Starting from an initial estimate for the model paramters we follow path of the steepest gardient to find the minimum of the log-likelihood function. Now Langevin Dynamics combines SGD with a stochastic parameter update in each step. Both contributions to the parameter updated are balanced usign hyper-parameters. Where the contribution of the gradient update becomes smaller with time (aka learning rate scheduling), while the stochastic update becomes more dominant. This eventually turns the optimization algorithm into a sampling algorithm. What is particulary nice about SGLD is that it draws a connection (or linearly interpolates) between SGD and Sampling.\n",
    "\n",
    "Instead of starting from a single value for the initial parameter distributions, we can also start with a (prior) distribution of the parameters. This will evolve an ensemble of points into the distribution determined by its score function.\n",
    "\n",
    "The principle was also illustrated in the original publication that introduced diffusion models, by Song et al. So in this notebook we will reproduce Fig. 2 of https://arxiv.org/abs/2011.13456\n",
    "\n",
    "Further References:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Stochastic_gradient_Langevin_dynamics\n",
    "- https://icml.cc/2011/papers/398_icmlpaper.pdf\n",
    "- Key read: https://yang-song.net/blog/2021/score/\n",
    "- https://github.com/yang-song/score_sde\n",
    "- https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import numpy as jnp\n",
    "from jax.scipy import stats\n",
    "import jax\n",
    "from functools import partial\n",
    "from jax import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, norm, mu, sigma):\n",
    "    return (\n",
    "        norm * jnp.exp(-0.5 * ((x - mu) / sigma) ** 2) / (sigma * jnp.sqrt(2 * np.pi))\n",
    "    )\n",
    "\n",
    "\n",
    "def gmm(x, norm, mu, sigma):\n",
    "    values = jnp.sum(gaussian(x, norm, mu, sigma), axis=0) / mu.shape[0]\n",
    "\n",
    "    if values.shape == (1,):\n",
    "        return values.reshape(())\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def log_gmm(x, norm, mu, sigma):\n",
    "    return jnp.log(gmm(x, norm, mu, sigma))\n",
    "\n",
    "\n",
    "norm, mu, sigma = (\n",
    "    jnp.array([1, 1])[:, None],\n",
    "    jnp.array([-1, 1])[:, None],\n",
    "    jnp.array([0.25, 0.25])[:, None],\n",
    ")\n",
    "\n",
    "x_plot = jnp.linspace(-2, 2, 1000)\n",
    "y = gmm(x_plot, norm, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_plot, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"p(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {},
   "source": [
    "In Jax we can directly get the score function by taking the gradient of the log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_log_part = partial(log_gmm, norm=norm, mu=mu, sigma=sigma)\n",
    "score_fun = jax.vmap(jax.grad(gmm_log_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "Now we can see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_plot, score_fun(x_plot))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"d/dx log p(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {},
   "source": [
    "Using a simple Python loop the algorithm would look like:\n",
    "\n",
    "```python\n",
    "\n",
    "n_iter = 200\n",
    "alpha_0 = 0.001\n",
    "p0 = 1 #0.9999\n",
    "beta = 1.\n",
    "n_samples = 100_000\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "x_init = random.normal(key, (n_samples,)) #jnp.zeros((n_samples,)) #\n",
    "\n",
    "x = x_init\n",
    "\n",
    "sample_trace_list = []\n",
    "\n",
    "for idx in range(n_iter):\n",
    "    key, subkey = random.split(key)\n",
    "    alpha = alpha_0 * (p0 ** idx) ** 2\n",
    "    x = x + alpha * score_fun(x) + jnp.sqrt(2 * alpha * beta) * random.normal(subkey, (n_samples,))\n",
    "    sample_trace_list.append(x)\n",
    "```\n",
    "\n",
    "Convince yourself that this is actually very similar to a \"normal\" ML training loop, where:\n",
    "\n",
    "- $\\alpha$ corresponds to the learning rate\n",
    "- We are using (optionally) a \"learning rate schedule\", determined by $\\alpha_0$ and $p_0$.\n",
    "\n",
    "The schedule however is only needed, once we use stochastic gradient decent. But in this example we evaluate the gradient of the log-likelihood (score) on the full batch, so we can set $p_0=0$.\n",
    "\n",
    "To make the code more efficient we replace the Python loop by a call to [`jax.lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan), which is exactly made for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple(\"Args\", [\"key\", \"idx\", \"x\", \"alpha_0\", \"p_0\", \"beta\"])\n",
    "\n",
    "\n",
    "def sample(score, args, _):\n",
    "    key, subkey = random.split(args.key)\n",
    "    alpha = args.alpha_0 * (args.p_0**args.idx) ** 2\n",
    "    dx = random.normal(subkey, args.x.shape)\n",
    "    x = args.x + alpha * score(args.x) + jnp.sqrt(2 * alpha * args.beta) * dx\n",
    "    return Args(key, args.idx + 1, x, args.alpha_0, args.p_0, args.beta), x\n",
    "\n",
    "\n",
    "n_samples = 100_000\n",
    "n_iter = 200\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "init = Args(\n",
    "    key=key,\n",
    "    idx=0,\n",
    "    x=random.normal(subkey, (n_samples,)),\n",
    "    alpha_0=0.001,\n",
    "    p_0=1,  # 0.9999\n",
    "    beta=1.0,\n",
    ")\n",
    "\n",
    "result, sample_trace = jax.lax.scan(partial(sample, score_fun), init, length=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {},
   "source": [
    "I have found it a good practice to use named tuples to handle the arguments of the body functions. Named tuples are automatically handled as PyTrees by jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(result.x, bins=200, density=True, histtype=\"step\", label=\"Transformed samples\")\n",
    "plt.hist(init.x, bins=100, density=True, histtype=\"step\", label=\"Initial samples\")\n",
    "plt.plot(x_plot, gmm(x_plot, norm, mu, sigma), label=\"True distribution\")\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {},
   "source": [
    "We have basically written a sampler, that can sample from any 1d distribution as long as we have its score function available! However, in practice you will find that the sampling is not very stable, meaning that the final distribution depends strongly on the choice of the hyper-parameters $\\alpha_0$, $p_0$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hist = partial(jnp.histogram, bins=100, range=(-2, 2), density=True)\n",
    "\n",
    "batched_histogram = jax.vmap(default_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diffusion_trace(trace, n_traces=5, ax=None, x_min=-2, x_max=2):\n",
    "    hist_values, hist_edges = batched_histogram(trace)\n",
    "\n",
    "    n_iter, n_samples = trace.shape\n",
    "\n",
    "    ax = plt.subplot() or ax\n",
    "    ax.imshow(\n",
    "        hist_values.T[:, :],\n",
    "        extent=[0, n_iter, x_min, x_max],\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "\n",
    "    # plot some example traces\n",
    "    key = random.PRNGKey(9823)\n",
    "    for idx in random.randint(key, (n_traces,), 0, n_samples):\n",
    "        ax.plot(trace[:, idx])\n",
    "\n",
    "    ax.set_ylim(x_min, x_max)\n",
    "    ax.set_xlabel(\"# Iteration\")\n",
    "    ax.set_ylabel(\"x\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "plot_diffusion_trace(sample_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {},
   "source": [
    "Now we take a look at the inverse process, which is the actual diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_ = 500_000\n",
    "\n",
    "x_init = (\n",
    "    sigma\n",
    "    * random.normal(\n",
    "        key,\n",
    "        (\n",
    "            2,\n",
    "            n_samples_ // 2,\n",
    "        ),\n",
    "    )\n",
    "    + mu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ax = plt.subplot()\n",
    "_ax.hist(\n",
    "    x_init.flatten(), bins=100, density=True, histtype=\"step\", label=\"Initial samples\"\n",
    ")\n",
    "_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_t = jnp.linspace(0, 1, n_iter)\n",
    "x = x_init.flatten()\n",
    "\n",
    "sample_trace_diffusion_beta = []\n",
    "\n",
    "for idx, _beta in enumerate(beta_t):\n",
    "    _key, _sub_key = random.split(key)\n",
    "    x = jnp.sqrt(1.0 - _beta) * x + jnp.sqrt(_beta) * random.normal(\n",
    "        key=_sub_key, shape=x.shape\n",
    "    )\n",
    "    sample_trace_diffusion_beta.append(x)\n",
    "\n",
    "sample_trace_diffusion_beta = jnp.stack(sample_trace_diffusion_beta, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diffusion_trace(sample_trace_diffusion_beta, x_min=-4, x_max=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {},
   "source": [
    "We can quickly verify that the distribution corresponds to a unit (normal) Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {x.mean():.2f}\")\n",
    "print(f\"Variance: {x.var():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ax = plt.subplot()\n",
    "_ax.hist(x, bins=100, density=True, histtype=\"step\", label=\"Diffused samples\")\n",
    "_ax.set_xlabel(\"x\")\n",
    "_ax.set_ylabel(\"p(x)\")\n",
    "_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
